{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "sKsRALyBRsBs",
        "QPMdQXKUktov",
        "hYJzcWiuquYh",
        "OXe908PrM9ME"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Language Models"
      ],
      "metadata": {
        "id": "BF-x1WWf-UYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we explore convolutional language models, i.e., language models that make use of convolutional layers."
      ],
      "metadata": {
        "id": "E6MlgetKCN9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolutions and Convolutional Layers"
      ],
      "metadata": {
        "id": "V2ACG6yjJ9rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to understand convolutional language models, we first need to undrestand convolutional layers. What's a convolutional layer? A convolutional layer is nothing more than a parameterized convolution. What's a convolution? Well, convolutions can be approached from various perspectives (e.g. signal processing, probability theory etc), however most often in deep learning, convolutions provide a way to search for occurances of a particular pattern within a larger signal. They can be seen as sliding window feature detectors (as apposed to the global window features detectors of MLPs). This is implemented as follows:\n",
        "\n",
        "- Let $k : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbf{R}^+$ be a kernel (i.e. a function giving some notion of *similarity* between vectors. Here, $n$ is the *window size* of the kernel.\n",
        "\n",
        "- Let $\\mathbf{s} \\in \\mathbb{R}^m$ be a signal.\n",
        "\n",
        "- The convolution of $\\mathbf{s}$ with $k(\\cdot, \\cdot)$ is given by sliding the kernel across $\\mathbf{s}$, producing a new *convolved* signal. The convolved signal will have higher levels in locations where $\\mathbf{s}$ had features corresponding to what the kernel was looking for.\n",
        "\n",
        "Most often, the kernel is implemented by computing the innenr product between the current window and a weight vector $\\mathbf{f}$. The weights can then be interpreted much like those of a linear layer, each corresponding to linear correlations between the features, and the kernel output. This process can be generalized to multi-dimensional and multi-channel signals. For instance, when considering images, we have signals with $C$ channels and two spatial dimensions. In this case, the kernel is typically specified using a *cube* of weights. Each slice corresponds to a channel. The product-sum still results in a scalar score however. In convolutional neural networks, we often run multiple such kernels over a given signal, producing another multi-channel 2D signal. Hence, convolutional layers can be stacked to develop rich hierarchical representations of high dimensional data."
      ],
      "metadata": {
        "id": "Wulq2yacJ_1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Hyperparameters: Kernel Size, Padding, Stride, Dialation, Groups"
      ],
      "metadata": {
        "id": "OEqoZZFikMqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beyond the basic convolution operation described above, deep learning frameworks such as PyTorch provide a number of additional hyperparameters that can be varied for specific applications. These typically include: kernel size, padding, stride, dialation and groups. We'll discuss each of these in turn.\n"
      ],
      "metadata": {
        "id": "Er0TRjU9kSIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.1. Kernel Size"
      ],
      "metadata": {
        "id": "UdD6_scAknE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel size is perhaps the simplest hyperparameter to understand. It simply describes the spatial dimensions of the kernel that's used to slide accross the input signal. In the diagram below, a kernel of size 3x3 is slid accross a 7x7 image (note that the channel dimension is not shown)."
      ],
      "metadata": {
        "id": "v8j7EW83mFqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/xLkpMs0/full-padding-no-strides-transposed-3633935968.gif)"
      ],
      "metadata": {
        "id": "wAbyPYpqlL1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the convolved signal lower resolution than the input signal. This comes from the fact that once the kernel reaches the end of one spatial dimension, it does not \"run over\" the edge. Specifically, if we have a kernel of size $(N, M)$ and we slide it over a $(K, L)$ sized signal (one unit per step), then the output size is:\n",
        "\n",
        "$$\\left(K - N + 1, L - M + 1\\right).$$\n",
        "\n",
        "To see why, consider the starting position of the kernel. At this point, there are $K - N$ horizontal spaces left to move an $L - M$ vertical spaces left to move. In this case, the total number of times we can apply the kernel is just 1 + each of those shifts. This gives the expression above."
      ],
      "metadata": {
        "id": "ZlFg9MelnbvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2. Padding"
      ],
      "metadata": {
        "id": "7UjQw3MUoQBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases, we may wish to have a little more *explicit* control over the size of the output. *Padding* can help with this by allowing the kernel to \"run over the edge\" of the input signal. This is often used to keep the spatial dimensions identical before and after applying a convolution. Consider again the previous equation:\n",
        "\n",
        "$$(K - N + 1, L - M + 1).$$\n",
        "\n",
        "In order to retain the input dimensions, we need to essentially add $N - 1$ and $M - 1$ extra shifts via padding. In the diagram this has been done with a kernel of size $(3, 3)$ and a signal of size $(5, 5)$. Applying what we've just said, there should be $3 - 1 = 2$ units of padding added to the signal in order to retain its dimensions. Indeed, we see this below, with one unit added on either side."
      ],
      "metadata": {
        "id": "_wc3icJ0oWO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://i.ibb.co/yFGdKG5/image12-2355761825.gif)"
      ],
      "metadata": {
        "id": "VfLGotqhqqDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in deep learning frameworks, padding is usually assumed to to symmetric. In this case, `padding = 1` indicates adding one unit of padding to *each side* of the input signal."
      ],
      "metadata": {
        "id": "Y_nMBIe1qsLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another commonly desired behaviour is for every partial or complete superimposition of the kernel to be accounted for. In this case, we need to add enough padding for so that a the kernel can be pushed until it almost \"falls off the edge.\""
      ],
      "metadata": {
        "id": "ez9TVgu47IIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/pjv9d09/Capture.png)"
      ],
      "metadata": {
        "id": "AxCd2b4H7x_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is usually called *full padding*, as apposed to *half* or *same padding*, as described in the previous section. For full padding, the output should have size $(K + N - 1, L + M - 1)$. Hence, the amount of padding required can be worked out as $2N - 2$ and $2M - 2$ respectively. In the diagram above, we have $2N - 2 = 2\\times 3 - 2 = 4$ split into symmetric padding of $2$ on either side."
      ],
      "metadata": {
        "id": "JHnw02JL8L8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3. Stride"
      ],
      "metadata": {
        "id": "N1EDX_YgrX5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another key hyperparameter is *stride*. So far we've only shown convolutions where the kernel is shifted one unit at each step. However, there's nothing stopping us from increasing or decreasing it. The diagram below shows a convolution with kernel size $(2, 2)$, no padding and a *stride* of two."
      ],
      "metadata": {
        "id": "iidOvocxrber"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/mHx6WFG/8ebb38993cb39631ee16a7ae27904381-4158148158.gif)\n"
      ],
      "metadata": {
        "id": "eor5sCzkt2pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using a stride greater than one, the spatial dimensions of the output are more dramatically reduced compared to the input. Specifically, an input of size $(K, L)$ with symmetric padding $P$ and a kernel of size $(N, M)$ with stride $S$ gives an output of size\n",
        "\n",
        "$$\\left(\\text{floor}\\left(\\dfrac{K + 2P - N}{S}\\right) + 1, \\text{floor}\\left(\\dfrac{L + 2P - M}{S}\\right) + 1\\right).$$\n",
        "\n",
        "So for the example above, we have $K = L = 5$ and $N = M = 3$. We also have $P = 0$ and $S = 2$. Then, the output has size\n",
        "\n",
        "$$\n",
        "    \\begin{align}\n",
        "\\left(\\text{floor}\\left(\\dfrac{5 + 0 - 3}{2}\\right) + 1, \\text{floor}\\left(\\dfrac{5 + 0 - 3}{2}\\right) + 1\\right) &= \\left(\\text{floor}\\left(\\dfrac{2}{2}\\right) + 1, \\text{floor}\\left(\\dfrac{2}{2}\\right) + 1\\right)\\\\ &= (2, 2).\n",
        "    \\end{align}\n",
        "$$\n",
        "\n",
        "This matches the diagram!"
      ],
      "metadata": {
        "id": "KhT-OPpSt4kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We mentioned that the stride can be increased *or decreased*. How can we decrease the stride below one? Well, we can actually have *fractional strides*. In fractional strides, we image padding *between* the levels of the input signal. This is best understood visually."
      ],
      "metadata": {
        "id": "cGfQs6wUwqay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/ncz2qdK/2aSir.gif)"
      ],
      "metadata": {
        "id": "fwr3xhswxJAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This diagram above shows a 3x3 kernel with a *fractional stride* of $1/2$. Due to the virtual padding between input levels, it only moves $1/2$ of a unit in every step. Therefore it has fractional stride. Fractional strides are frequently used in *transposed convolutions*. When we unroll a convolution into a single linear transformation, the transpose of that matrix is the transposed convolution. Transposed convolutions are spatially inverted convolutions. If we have a convolution mapping inputs of size $(K, L)$ to outputs of size $(K', L')$, then its transpose does the inverse. Note however that transposed convolutions are not *functional* inverses."
      ],
      "metadata": {
        "id": "B6PluYwexZnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.4. Groups"
      ],
      "metadata": {
        "id": "0iS9iOaniBbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouped convolutions split the input feature map into $k$ chunks along the channel dimension. Then, $k$ filter groups are learned for each chunk, producing $k$ output feature maps. The output feature maps are then combined to produce a single feature map. The primary effect of grouping is increased sparsity (since each filter group can only access part of the input feature map); as well as increased model paralellism.\n",
        "\n",
        "![](https://i.ibb.co/kQbXybf/groups.png)"
      ],
      "metadata": {
        "id": "45j0q3Eem-TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Causal Convolutions"
      ],
      "metadata": {
        "id": "ahWAQKXQpdJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoregressive language models are inherently causal in nature - past observations can influence future observations, however *not* visa-vera. In MLP-based models, this can be achieved applying masks to the weights of each layer such that there's no information flow from future to past. A similar approach can be taken for CNNs using *causal convolutions*.\n",
        "\n",
        "![](https://i.ibb.co/ydRHCvc/causal.png)\n",
        "\n",
        "In a standard convolutional layer, information flows bidirectionaly. For instance, in figure (a), the purple units have access to information from two past *and* future positions. However, in order to enforce causality we need to ensure that units only have access to *past* positions. This change is shown in figure (b)."
      ],
      "metadata": {
        "id": "tmiMXefwr-nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice this can be achived by *padding* the input sequence before applying the convolution.\n",
        "\n",
        "![](https://i.ibb.co/0ZwjsTk/padding.jpg)\n",
        "\n",
        "In this instance, padding the input with two zeros has the effect of re-aligning the input and output feature map such that $x_i'$ is influenced only by $x_{j \\le i}$. The final two units are also removed as they don't correspond to any input position. Of course, this isn't yet causality preserving as $x_i$ can influence $x_i'$. To fix this, we can simply shift the input one more space to the right.\n",
        "\n",
        "![](https://i.ibb.co/0FWyBCW/padding.png)\n",
        "\n",
        "The specific amount of padding required is the kernel size minus one (for the input layer it's just the kernel size). Also, the notice that as we increase the depth of the stack, the receptive field (context size) increases. While the first hidden layer units can only condition on two previous positions, the second hidden layer units can condition on four. This differs from causal MLPs which generally have at least one unit able to condition on all past positions."
      ],
      "metadata": {
        "id": "g6IX3Ld_4JfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with that we're done; we now have a mechanism for preserving causality in CNNs."
      ],
      "metadata": {
        "id": "Pfjy6sC8A4Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Dialated Causal Convolutions\n"
      ],
      "metadata": {
        "id": "h7GnVmYeCoeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One extension to the above scheme is to increase the receptive field of the stack through dialated convolutions. Dialation increases the receptive field without requiring additional layers.\n",
        "\n",
        "![](https://i.ibb.co/FmyT92B/dialated.png)\n",
        "\n",
        "In the above diagram, the convolutional layers are set to have exponentially increasing dialation from $2^0$ to $2^3$.\n",
        "\n",
        "> Additionally the stride is set so as to ignore unconnected units (e.g., some units of the feature map in hidden layer 1 are unconnected to the input, so we ignore these in hidden layer 2 by using an appropriate stride). We do the same for the remaining layers.\n",
        "\n",
        "When working with dialated and/or strided causal convolutions, the causal padding must be adapted accordingly."
      ],
      "metadata": {
        "id": "8y7dOiRbCuDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Multi-dimensional Causal Convolutions"
      ],
      "metadata": {
        "id": "OLsZ-7c0JOeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've constrained discussion to 1-dimensional convolutions. These are appropriate for sequence modelling. However, modalities such as images can also be considered to have two spatial dimensions. In this case certain relationships may be easier to learn if we consider past positions along both spatial axes. This is the approch taken by models such as PixelCNN."
      ],
      "metadata": {
        "id": "s4fxReZeJYbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To preserve causality in higher dimensions, we simply extend the padding scheme to the relevant number of axes. For instance, in 2D we can pad both the left and top side of each feature map by (e.g. by `kernel_size - 1`). The following is an exerpt from the PixelCNN paper.\n",
        "\n",
        "![](https://i.ibb.co/wcKpq7v/exerpt.png)\n",
        "\n",
        "We can see that their approach is a direct extension of the 1D masking scheme to two dimensions. They mention that the masks can be implemented by zeroing the kernel weights. This is indeed an alternative way to implement masks rather than padding."
      ],
      "metadata": {
        "id": "zBDTscXmLqub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementation"
      ],
      "metadata": {
        "id": "7ILbqKFgI4TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Generating IMDb Reviews"
      ],
      "metadata": {
        "id": "UZfnJUaVQOO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we leverage 1D causal convolutions to train an autoregressive language model on IMDb reviews."
      ],
      "metadata": {
        "id": "5jIfoOE4QMX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1. Download the IMDb Reviews Dataset"
      ],
      "metadata": {
        "id": "sKsRALyBRsBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "!pip -q install nltk normality"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0q5vb7jiTuqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "CInaQOH3TJPw",
        "outputId": "574f9050-425e-4965-c6ea-14200a0288a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2. Preprocess and Tokenize the Reviews"
      ],
      "metadata": {
        "id": "QPMdQXKUktov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "!pip -q install tokenizers"
      ],
      "metadata": {
        "id": "1q55LD0kp4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "from normality import normalize\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "positive_file_ids = movie_reviews.fileids('pos')\n",
        "negative_file_ids = movie_reviews.fileids('neg')\n",
        "\n",
        "keep_punctuation = {\n",
        "    '. ': 'keepperiod',\n",
        "    ', ': 'keepcomma',\n",
        "    '!': 'keepexclamation',\n",
        "    '\\'': 'keepquote',\n",
        "}\n",
        "\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "\n",
        "    for character, placeholder in keep_punctuation.items():\n",
        "        text = text.replace(character, placeholder)\n",
        "\n",
        "    text = normalize(text)\n",
        "\n",
        "    for character, placeholder in keep_punctuation.items():\n",
        "        text = text.replace(f' {placeholder} ', character)\n",
        "        text = text.replace(f' {placeholder}', character)\n",
        "        text = text.replace(f'{placeholder} ', character)\n",
        "        text = text.replace(f'{placeholder}', character)\n",
        "\n",
        "    return text\n",
        "\n",
        "# For tokenization we use SentencePiece\n",
        "\n",
        "corpus = []\n",
        "\n",
        "print('Generating corpus...')\n",
        "\n",
        "for file_id in tqdm(positive_file_ids):\n",
        "    corpus.append(preprocess(movie_reviews.raw(file_id)))\n",
        "\n",
        "print('Training tokenizer...')\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer.train_from_iterator(corpus, vocab_size=5000)\n",
        "tokenizer.add_special_tokens(['<bos>', '<eos>', '<pad>'])\n",
        "tokenizer.save('./tokenizer.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-bWlIVngQho",
        "outputId": "c92d60ee-bd96-400b-9f9e-9a8c404ffe5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 475.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.3. Define the Model"
      ],
      "metadata": {
        "id": "hYJzcWiuquYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CausalCNNConfiguration:\n",
        "    \"\"\"Causal CNN configuration.\"\"\"\n",
        "\n",
        "    vocabulary_size: int\n",
        "    embedding_dimension: int\n",
        "    channels: int\n",
        "    kernel_size: int\n",
        "    pad_id: int\n",
        "\n",
        "\n",
        "class CausalCNN(nn.Module):\n",
        "    \"\"\"Causal CNN.\"\"\"\n",
        "\n",
        "    def __init__(self, configuration: CausalCNNConfiguration) -> None:\n",
        "        \"\"\"Initializes the module.\"\"\"\n",
        "\n",
        "        super(CausalCNN, self).__init__()\n",
        "\n",
        "        self.configuration = configuration\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=configuration.vocabulary_size, embedding_dim=configuration.embedding_dimension, padding_idx=configuration.pad_id)\n",
        "\n",
        "        self.convolution_0 = nn.Conv1d(in_channels=configuration.embedding_dimension, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)  # We want to handle padding manually.\n",
        "        self.convolution_1 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "        self.convolution_2 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "        self.convolution_3 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "        self.convolution_4 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "        self.convolution_5 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.channels, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "        self.convolution_6 = nn.Conv1d(in_channels=configuration.channels, out_channels=configuration.vocabulary_size, kernel_size=configuration.kernel_size, stride=1, padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        padding = (self.configuration.kernel_size - 1, 0, 0, 0)\n",
        "        input_padding = (self.configuration.kernel_size, -1)\n",
        "\n",
        "        x = F.pad(x, input_padding, value=self.configuration.pad_id)  # Pad and shift right.\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(-2, -1)  # Convert from (B, L, C) to (B, C, L)\n",
        "        x = self.convolution_0(x)\n",
        "\n",
        "        x = F.relu(self.convolution_1(F.pad(x, padding)))\n",
        "        x = F.relu(self.convolution_2(F.pad(x, padding)))\n",
        "        x = F.relu(self.convolution_3(F.pad(x, padding)))\n",
        "        x = F.relu(self.convolution_4(F.pad(x, padding)))\n",
        "        x = F.relu(self.convolution_5(F.pad(x, padding)))\n",
        "        x = self.convolution_6(F.pad(x, padding))\n",
        "\n",
        "        logits = F.log_softmax(x.transpose(-2, -1), dim=-1)  # Convert from (B, C, L) to (B, L, C)\n",
        "                                      # Can also be done with x.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "yIc1eTneqzzg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "configuration = CausalCNNConfiguration(\n",
        "    vocabulary_size = tokenizer.get_vocab_size(),\n",
        "    embedding_dimension = 256,\n",
        "    channels=128,\n",
        "    kernel_size=3,\n",
        "    pad_id=tokenizer.token_to_id('<pad>'),\n",
        ")\n",
        "\n",
        "model = CausalCNN(configuration).cuda()"
      ],
      "metadata": {
        "id": "xSfREonunUPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.4. Train the Model"
      ],
      "metadata": {
        "id": "OXe908PrM9ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "A1rQVM5pPyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "epochs = 20\n",
        "sequences = len(corpus)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    losses = []\n",
        "\n",
        "    for sequence_index, sequence in enumerate(corpus):\n",
        "\n",
        "        tokens = tokenizer.encode(f'<bos>{sequence}<eos>')\n",
        "        tokens = torch.tensor(tokens.ids)\n",
        "        tokens = tokens.cuda()\n",
        "\n",
        "        sequence_length = tokens.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(tokens.view(1, -1)).view(sequence_length, -1)\n",
        "        loss = criterion(logits, tokens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if (sequence_index % 10) == 0:\n",
        "            mean_loss = sum(losses) / len(losses)\n",
        "            losses.clear()\n",
        "\n",
        "            print(f'Epoch {epoch}/{epochs}, Sequence {sequence_index}/{sequences} - loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbnK6jvMM_Lt",
        "outputId": "f29e51aa-0f62-4ab1-e20e-aea3313793c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20, Sequence 0/1000 - loss: 8.51842975616455\n",
            "Epoch 0/20, Sequence 10/1000 - loss: 7.877602577209473\n",
            "Epoch 0/20, Sequence 20/1000 - loss: 7.4164137840271\n",
            "Epoch 0/20, Sequence 30/1000 - loss: 7.476500988006592\n",
            "Epoch 0/20, Sequence 40/1000 - loss: 7.0692057609558105\n",
            "Epoch 0/20, Sequence 50/1000 - loss: 7.266458511352539\n",
            "Epoch 0/20, Sequence 60/1000 - loss: 7.045554161071777\n",
            "Epoch 0/20, Sequence 70/1000 - loss: 7.349377155303955\n",
            "Epoch 0/20, Sequence 80/1000 - loss: 7.187324047088623\n",
            "Epoch 0/20, Sequence 90/1000 - loss: 7.198117733001709\n",
            "Epoch 0/20, Sequence 100/1000 - loss: 7.118460178375244\n",
            "Epoch 0/20, Sequence 110/1000 - loss: 7.125805854797363\n",
            "Epoch 0/20, Sequence 120/1000 - loss: 7.254831790924072\n",
            "Epoch 0/20, Sequence 130/1000 - loss: 7.211723327636719\n",
            "Epoch 0/20, Sequence 140/1000 - loss: 7.159432411193848\n",
            "Epoch 0/20, Sequence 150/1000 - loss: 7.230100154876709\n",
            "Epoch 0/20, Sequence 160/1000 - loss: 6.9448442459106445\n",
            "Epoch 0/20, Sequence 170/1000 - loss: 7.242609977722168\n",
            "Epoch 0/20, Sequence 180/1000 - loss: 7.031035900115967\n",
            "Epoch 0/20, Sequence 190/1000 - loss: 7.14279842376709\n",
            "Epoch 0/20, Sequence 200/1000 - loss: 7.265684127807617\n",
            "Epoch 0/20, Sequence 210/1000 - loss: 6.976208686828613\n",
            "Epoch 0/20, Sequence 220/1000 - loss: 7.20844841003418\n",
            "Epoch 0/20, Sequence 230/1000 - loss: 7.157937049865723\n",
            "Epoch 0/20, Sequence 240/1000 - loss: 7.044393062591553\n",
            "Epoch 0/20, Sequence 250/1000 - loss: 6.938553333282471\n",
            "Epoch 0/20, Sequence 260/1000 - loss: 6.986313819885254\n",
            "Epoch 0/20, Sequence 270/1000 - loss: 7.02266788482666\n",
            "Epoch 0/20, Sequence 280/1000 - loss: 7.194140911102295\n",
            "Epoch 0/20, Sequence 290/1000 - loss: 7.018482685089111\n",
            "Epoch 0/20, Sequence 300/1000 - loss: 6.9950103759765625\n",
            "Epoch 0/20, Sequence 310/1000 - loss: 6.85964298248291\n",
            "Epoch 0/20, Sequence 320/1000 - loss: 7.193698883056641\n",
            "Epoch 0/20, Sequence 330/1000 - loss: 6.910903453826904\n",
            "Epoch 0/20, Sequence 340/1000 - loss: 7.149479389190674\n",
            "Epoch 0/20, Sequence 350/1000 - loss: 7.175396919250488\n",
            "Epoch 0/20, Sequence 360/1000 - loss: 7.131645679473877\n",
            "Epoch 0/20, Sequence 370/1000 - loss: 6.981359481811523\n",
            "Epoch 0/20, Sequence 380/1000 - loss: 6.949236869812012\n",
            "Epoch 0/20, Sequence 390/1000 - loss: 7.2827277183532715\n",
            "Epoch 0/20, Sequence 400/1000 - loss: 7.089303970336914\n",
            "Epoch 0/20, Sequence 410/1000 - loss: 7.107294082641602\n",
            "Epoch 0/20, Sequence 420/1000 - loss: 7.1239190101623535\n",
            "Epoch 0/20, Sequence 430/1000 - loss: 7.163373947143555\n",
            "Epoch 0/20, Sequence 440/1000 - loss: 6.868946075439453\n",
            "Epoch 0/20, Sequence 450/1000 - loss: 6.9257893562316895\n",
            "Epoch 0/20, Sequence 460/1000 - loss: 6.900580883026123\n",
            "Epoch 0/20, Sequence 470/1000 - loss: 6.867556095123291\n",
            "Epoch 0/20, Sequence 480/1000 - loss: 6.825237274169922\n",
            "Epoch 0/20, Sequence 490/1000 - loss: 6.972903251647949\n",
            "Epoch 0/20, Sequence 500/1000 - loss: 7.026374340057373\n",
            "Epoch 0/20, Sequence 510/1000 - loss: 6.964354515075684\n",
            "Epoch 0/20, Sequence 520/1000 - loss: 7.222442150115967\n",
            "Epoch 0/20, Sequence 530/1000 - loss: 6.936304092407227\n",
            "Epoch 0/20, Sequence 540/1000 - loss: 7.185015678405762\n",
            "Epoch 0/20, Sequence 550/1000 - loss: 6.824937343597412\n",
            "Epoch 0/20, Sequence 560/1000 - loss: 6.650382995605469\n",
            "Epoch 0/20, Sequence 570/1000 - loss: 6.8299031257629395\n",
            "Epoch 0/20, Sequence 580/1000 - loss: 6.849971771240234\n",
            "Epoch 0/20, Sequence 590/1000 - loss: 7.0022783279418945\n",
            "Epoch 0/20, Sequence 600/1000 - loss: 7.085689544677734\n",
            "Epoch 0/20, Sequence 610/1000 - loss: 6.8619065284729\n",
            "Epoch 0/20, Sequence 620/1000 - loss: 6.771350383758545\n",
            "Epoch 0/20, Sequence 630/1000 - loss: 6.83763313293457\n",
            "Epoch 0/20, Sequence 640/1000 - loss: 7.193943977355957\n",
            "Epoch 0/20, Sequence 650/1000 - loss: 6.9803266525268555\n",
            "Epoch 0/20, Sequence 660/1000 - loss: 6.953072547912598\n",
            "Epoch 0/20, Sequence 670/1000 - loss: 6.968283176422119\n",
            "Epoch 0/20, Sequence 680/1000 - loss: 6.611472129821777\n",
            "Epoch 0/20, Sequence 690/1000 - loss: 6.805172443389893\n",
            "Epoch 0/20, Sequence 700/1000 - loss: 6.585865020751953\n",
            "Epoch 0/20, Sequence 710/1000 - loss: 7.06496524810791\n",
            "Epoch 0/20, Sequence 720/1000 - loss: 6.987829208374023\n",
            "Epoch 0/20, Sequence 730/1000 - loss: 6.8223395347595215\n",
            "Epoch 0/20, Sequence 740/1000 - loss: 6.942032337188721\n",
            "Epoch 0/20, Sequence 750/1000 - loss: 6.758339881896973\n",
            "Epoch 0/20, Sequence 760/1000 - loss: 7.174320220947266\n",
            "Epoch 0/20, Sequence 770/1000 - loss: 7.00602912902832\n",
            "Epoch 0/20, Sequence 780/1000 - loss: 6.878936290740967\n",
            "Epoch 0/20, Sequence 790/1000 - loss: 6.903406620025635\n",
            "Epoch 0/20, Sequence 800/1000 - loss: 6.725432395935059\n",
            "Epoch 0/20, Sequence 810/1000 - loss: 7.008167743682861\n",
            "Epoch 0/20, Sequence 820/1000 - loss: 6.608885288238525\n",
            "Epoch 0/20, Sequence 830/1000 - loss: 7.015921115875244\n",
            "Epoch 0/20, Sequence 840/1000 - loss: 6.876029014587402\n",
            "Epoch 0/20, Sequence 850/1000 - loss: 6.687205791473389\n",
            "Epoch 0/20, Sequence 860/1000 - loss: 6.890873908996582\n",
            "Epoch 0/20, Sequence 870/1000 - loss: 6.8629984855651855\n",
            "Epoch 0/20, Sequence 880/1000 - loss: 6.82481050491333\n",
            "Epoch 0/20, Sequence 890/1000 - loss: 6.858835697174072\n",
            "Epoch 0/20, Sequence 900/1000 - loss: 6.331140041351318\n",
            "Epoch 0/20, Sequence 910/1000 - loss: 6.760436534881592\n",
            "Epoch 0/20, Sequence 920/1000 - loss: 7.161991119384766\n",
            "Epoch 0/20, Sequence 930/1000 - loss: 6.963937759399414\n",
            "Epoch 0/20, Sequence 940/1000 - loss: 6.833668231964111\n",
            "Epoch 0/20, Sequence 950/1000 - loss: 6.784912109375\n",
            "Epoch 0/20, Sequence 960/1000 - loss: 6.788233757019043\n",
            "Epoch 0/20, Sequence 970/1000 - loss: 7.137024402618408\n",
            "Epoch 0/20, Sequence 980/1000 - loss: 6.846560478210449\n",
            "Epoch 0/20, Sequence 990/1000 - loss: 6.754051685333252\n",
            "Epoch 1/20, Sequence 0/1000 - loss: 6.983222007751465\n",
            "Epoch 1/20, Sequence 10/1000 - loss: 6.9536590576171875\n",
            "Epoch 1/20, Sequence 20/1000 - loss: 6.709232807159424\n",
            "Epoch 1/20, Sequence 30/1000 - loss: 6.980381011962891\n",
            "Epoch 1/20, Sequence 40/1000 - loss: 6.603055477142334\n",
            "Epoch 1/20, Sequence 50/1000 - loss: 6.800522327423096\n",
            "Epoch 1/20, Sequence 60/1000 - loss: 6.526933670043945\n",
            "Epoch 1/20, Sequence 70/1000 - loss: 7.040347576141357\n",
            "Epoch 1/20, Sequence 80/1000 - loss: 6.684736728668213\n",
            "Epoch 1/20, Sequence 90/1000 - loss: 6.744366645812988\n",
            "Epoch 1/20, Sequence 100/1000 - loss: 6.668288707733154\n",
            "Epoch 1/20, Sequence 110/1000 - loss: 6.694627285003662\n",
            "Epoch 1/20, Sequence 120/1000 - loss: 6.801729202270508\n",
            "Epoch 1/20, Sequence 130/1000 - loss: 6.6705546379089355\n",
            "Epoch 1/20, Sequence 140/1000 - loss: 6.6718058586120605\n",
            "Epoch 1/20, Sequence 150/1000 - loss: 6.782948017120361\n",
            "Epoch 1/20, Sequence 160/1000 - loss: 6.410288333892822\n",
            "Epoch 1/20, Sequence 170/1000 - loss: 6.719154357910156\n",
            "Epoch 1/20, Sequence 180/1000 - loss: 6.486012935638428\n",
            "Epoch 1/20, Sequence 190/1000 - loss: 6.649656772613525\n",
            "Epoch 1/20, Sequence 200/1000 - loss: 6.823526859283447\n",
            "Epoch 1/20, Sequence 210/1000 - loss: 6.395247936248779\n",
            "Epoch 1/20, Sequence 220/1000 - loss: 6.653636455535889\n",
            "Epoch 1/20, Sequence 230/1000 - loss: 6.6896843910217285\n",
            "Epoch 1/20, Sequence 240/1000 - loss: 6.57255744934082\n",
            "Epoch 1/20, Sequence 250/1000 - loss: 6.466744422912598\n",
            "Epoch 1/20, Sequence 260/1000 - loss: 6.444280624389648\n",
            "Epoch 1/20, Sequence 270/1000 - loss: 6.550638198852539\n",
            "Epoch 1/20, Sequence 280/1000 - loss: 6.609226226806641\n",
            "Epoch 1/20, Sequence 290/1000 - loss: 6.518552780151367\n",
            "Epoch 1/20, Sequence 300/1000 - loss: 6.4746479988098145\n",
            "Epoch 1/20, Sequence 310/1000 - loss: 6.287697792053223\n",
            "Epoch 1/20, Sequence 320/1000 - loss: 6.66139030456543\n",
            "Epoch 1/20, Sequence 330/1000 - loss: 6.425766468048096\n",
            "Epoch 1/20, Sequence 340/1000 - loss: 6.617927551269531\n",
            "Epoch 1/20, Sequence 350/1000 - loss: 6.676873207092285\n",
            "Epoch 1/20, Sequence 360/1000 - loss: 6.622617244720459\n",
            "Epoch 1/20, Sequence 370/1000 - loss: 6.513538837432861\n",
            "Epoch 1/20, Sequence 380/1000 - loss: 6.443587779998779\n",
            "Epoch 1/20, Sequence 390/1000 - loss: 6.775943756103516\n",
            "Epoch 1/20, Sequence 400/1000 - loss: 6.515641212463379\n",
            "Epoch 1/20, Sequence 410/1000 - loss: 6.669983386993408\n",
            "Epoch 1/20, Sequence 420/1000 - loss: 6.630390644073486\n",
            "Epoch 1/20, Sequence 430/1000 - loss: 6.649669647216797\n",
            "Epoch 1/20, Sequence 440/1000 - loss: 6.356310844421387\n",
            "Epoch 1/20, Sequence 450/1000 - loss: 6.37901496887207\n",
            "Epoch 1/20, Sequence 460/1000 - loss: 6.507001876831055\n",
            "Epoch 1/20, Sequence 470/1000 - loss: 6.324481964111328\n",
            "Epoch 1/20, Sequence 480/1000 - loss: 6.332266807556152\n",
            "Epoch 1/20, Sequence 490/1000 - loss: 6.388132095336914\n",
            "Epoch 1/20, Sequence 500/1000 - loss: 6.5763115882873535\n",
            "Epoch 1/20, Sequence 510/1000 - loss: 6.545271396636963\n",
            "Epoch 1/20, Sequence 520/1000 - loss: 6.720368385314941\n",
            "Epoch 1/20, Sequence 530/1000 - loss: 6.431823253631592\n",
            "Epoch 1/20, Sequence 540/1000 - loss: 6.75421667098999\n",
            "Epoch 1/20, Sequence 550/1000 - loss: 6.40350866317749\n",
            "Epoch 1/20, Sequence 560/1000 - loss: 6.17845344543457\n",
            "Epoch 1/20, Sequence 570/1000 - loss: 6.364790916442871\n",
            "Epoch 1/20, Sequence 580/1000 - loss: 6.360462665557861\n",
            "Epoch 1/20, Sequence 590/1000 - loss: 6.577155590057373\n",
            "Epoch 1/20, Sequence 600/1000 - loss: 6.6486639976501465\n",
            "Epoch 1/20, Sequence 610/1000 - loss: 6.388712406158447\n",
            "Epoch 1/20, Sequence 620/1000 - loss: 6.289461135864258\n",
            "Epoch 1/20, Sequence 630/1000 - loss: 6.4056878089904785\n",
            "Epoch 1/20, Sequence 640/1000 - loss: 6.7745866775512695\n",
            "Epoch 1/20, Sequence 650/1000 - loss: 6.506069660186768\n",
            "Epoch 1/20, Sequence 660/1000 - loss: 6.545590400695801\n",
            "Epoch 1/20, Sequence 670/1000 - loss: 6.51542329788208\n",
            "Epoch 1/20, Sequence 680/1000 - loss: 6.124054431915283\n",
            "Epoch 1/20, Sequence 690/1000 - loss: 6.20164155960083\n",
            "Epoch 1/20, Sequence 700/1000 - loss: 6.049150466918945\n",
            "Epoch 1/20, Sequence 710/1000 - loss: 6.579015254974365\n",
            "Epoch 1/20, Sequence 720/1000 - loss: 6.471193313598633\n",
            "Epoch 1/20, Sequence 730/1000 - loss: 6.34218168258667\n",
            "Epoch 1/20, Sequence 740/1000 - loss: 6.471096992492676\n",
            "Epoch 1/20, Sequence 750/1000 - loss: 6.303327560424805\n",
            "Epoch 1/20, Sequence 760/1000 - loss: 6.712298393249512\n",
            "Epoch 1/20, Sequence 770/1000 - loss: 6.516491413116455\n",
            "Epoch 1/20, Sequence 780/1000 - loss: 6.410037040710449\n",
            "Epoch 1/20, Sequence 790/1000 - loss: 6.443427562713623\n",
            "Epoch 1/20, Sequence 800/1000 - loss: 6.212832450866699\n",
            "Epoch 1/20, Sequence 810/1000 - loss: 6.568470001220703\n",
            "Epoch 1/20, Sequence 820/1000 - loss: 6.130203723907471\n",
            "Epoch 1/20, Sequence 830/1000 - loss: 6.546087741851807\n",
            "Epoch 1/20, Sequence 840/1000 - loss: 6.371428489685059\n",
            "Epoch 1/20, Sequence 850/1000 - loss: 6.070528030395508\n",
            "Epoch 1/20, Sequence 860/1000 - loss: 6.433943271636963\n",
            "Epoch 1/20, Sequence 870/1000 - loss: 6.390876770019531\n",
            "Epoch 1/20, Sequence 880/1000 - loss: 6.380749225616455\n",
            "Epoch 1/20, Sequence 890/1000 - loss: 6.383361339569092\n",
            "Epoch 1/20, Sequence 900/1000 - loss: 5.858741283416748\n",
            "Epoch 1/20, Sequence 910/1000 - loss: 6.3044867515563965\n",
            "Epoch 1/20, Sequence 920/1000 - loss: 6.685317516326904\n",
            "Epoch 1/20, Sequence 930/1000 - loss: 6.507780075073242\n",
            "Epoch 1/20, Sequence 940/1000 - loss: 6.408647537231445\n",
            "Epoch 1/20, Sequence 950/1000 - loss: 6.293049335479736\n",
            "Epoch 1/20, Sequence 960/1000 - loss: 6.375288486480713\n",
            "Epoch 1/20, Sequence 970/1000 - loss: 6.719599723815918\n",
            "Epoch 1/20, Sequence 980/1000 - loss: 6.383216381072998\n",
            "Epoch 1/20, Sequence 990/1000 - loss: 6.259662628173828\n",
            "Epoch 2/20, Sequence 0/1000 - loss: 6.5715436935424805\n",
            "Epoch 2/20, Sequence 10/1000 - loss: 6.483627796173096\n",
            "Epoch 2/20, Sequence 20/1000 - loss: 6.268523216247559\n",
            "Epoch 2/20, Sequence 30/1000 - loss: 6.579570293426514\n",
            "Epoch 2/20, Sequence 40/1000 - loss: 6.16095495223999\n",
            "Epoch 2/20, Sequence 50/1000 - loss: 6.4470906257629395\n",
            "Epoch 2/20, Sequence 60/1000 - loss: 6.097649574279785\n",
            "Epoch 2/20, Sequence 70/1000 - loss: 6.676753044128418\n",
            "Epoch 2/20, Sequence 80/1000 - loss: 6.251550674438477\n",
            "Epoch 2/20, Sequence 90/1000 - loss: 6.388845443725586\n",
            "Epoch 2/20, Sequence 100/1000 - loss: 6.142930507659912\n",
            "Epoch 2/20, Sequence 110/1000 - loss: 6.3417534828186035\n",
            "Epoch 2/20, Sequence 120/1000 - loss: 6.411558151245117\n",
            "Epoch 2/20, Sequence 130/1000 - loss: 6.182798385620117\n",
            "Epoch 2/20, Sequence 140/1000 - loss: 6.300896644592285\n",
            "Epoch 2/20, Sequence 150/1000 - loss: 6.376856327056885\n",
            "Epoch 2/20, Sequence 160/1000 - loss: 6.083864212036133\n",
            "Epoch 2/20, Sequence 170/1000 - loss: 6.237598896026611\n",
            "Epoch 2/20, Sequence 180/1000 - loss: 5.954070568084717\n",
            "Epoch 2/20, Sequence 190/1000 - loss: 6.290389060974121\n",
            "Epoch 2/20, Sequence 200/1000 - loss: 6.458301544189453\n",
            "Epoch 2/20, Sequence 210/1000 - loss: 5.952613830566406\n",
            "Epoch 2/20, Sequence 220/1000 - loss: 6.38145112991333\n",
            "Epoch 2/20, Sequence 230/1000 - loss: 6.291812419891357\n",
            "Epoch 2/20, Sequence 240/1000 - loss: 6.212078094482422\n",
            "Epoch 2/20, Sequence 250/1000 - loss: 6.15806245803833\n",
            "Epoch 2/20, Sequence 260/1000 - loss: 6.040402889251709\n",
            "Epoch 2/20, Sequence 270/1000 - loss: 6.140106201171875\n",
            "Epoch 2/20, Sequence 280/1000 - loss: 6.259578227996826\n",
            "Epoch 2/20, Sequence 290/1000 - loss: 6.227736949920654\n",
            "Epoch 2/20, Sequence 300/1000 - loss: 6.097724914550781\n",
            "Epoch 2/20, Sequence 310/1000 - loss: 5.858294486999512\n",
            "Epoch 2/20, Sequence 320/1000 - loss: 6.280147552490234\n",
            "Epoch 2/20, Sequence 330/1000 - loss: 6.022323131561279\n",
            "Epoch 2/20, Sequence 340/1000 - loss: 6.335471153259277\n",
            "Epoch 2/20, Sequence 350/1000 - loss: 6.332230091094971\n",
            "Epoch 2/20, Sequence 360/1000 - loss: 6.294225215911865\n",
            "Epoch 2/20, Sequence 370/1000 - loss: 6.151131629943848\n",
            "Epoch 2/20, Sequence 380/1000 - loss: 6.0907464027404785\n",
            "Epoch 2/20, Sequence 390/1000 - loss: 6.404336929321289\n",
            "Epoch 2/20, Sequence 400/1000 - loss: 6.048938274383545\n",
            "Epoch 2/20, Sequence 410/1000 - loss: 6.388731956481934\n",
            "Epoch 2/20, Sequence 420/1000 - loss: 6.350341320037842\n",
            "Epoch 2/20, Sequence 430/1000 - loss: 6.339563369750977\n",
            "Epoch 2/20, Sequence 440/1000 - loss: 6.012519359588623\n",
            "Epoch 2/20, Sequence 450/1000 - loss: 6.010128021240234\n",
            "Epoch 2/20, Sequence 460/1000 - loss: 6.182243347167969\n",
            "Epoch 2/20, Sequence 470/1000 - loss: 5.983940601348877\n",
            "Epoch 2/20, Sequence 480/1000 - loss: 5.992337226867676\n",
            "Epoch 2/20, Sequence 490/1000 - loss: 6.0583014488220215\n",
            "Epoch 2/20, Sequence 500/1000 - loss: 6.297893047332764\n",
            "Epoch 2/20, Sequence 510/1000 - loss: 6.316562652587891\n",
            "Epoch 2/20, Sequence 520/1000 - loss: 6.426962852478027\n",
            "Epoch 2/20, Sequence 530/1000 - loss: 6.169114112854004\n",
            "Epoch 2/20, Sequence 540/1000 - loss: 6.4931206703186035\n",
            "Epoch 2/20, Sequence 550/1000 - loss: 6.0560078620910645\n",
            "Epoch 2/20, Sequence 560/1000 - loss: 5.854797840118408\n",
            "Epoch 2/20, Sequence 570/1000 - loss: 6.053679943084717\n",
            "Epoch 2/20, Sequence 580/1000 - loss: 6.053533554077148\n",
            "Epoch 2/20, Sequence 590/1000 - loss: 6.297517776489258\n",
            "Epoch 2/20, Sequence 600/1000 - loss: 6.439163684844971\n",
            "Epoch 2/20, Sequence 610/1000 - loss: 6.045703887939453\n",
            "Epoch 2/20, Sequence 620/1000 - loss: 5.967196941375732\n",
            "Epoch 2/20, Sequence 630/1000 - loss: 6.189695835113525\n",
            "Epoch 2/20, Sequence 640/1000 - loss: 6.39638090133667\n",
            "Epoch 2/20, Sequence 650/1000 - loss: 6.201571464538574\n",
            "Epoch 2/20, Sequence 660/1000 - loss: 6.294875144958496\n",
            "Epoch 2/20, Sequence 670/1000 - loss: 6.261847972869873\n",
            "Epoch 2/20, Sequence 680/1000 - loss: 5.795147895812988\n",
            "Epoch 2/20, Sequence 690/1000 - loss: 5.768171787261963\n",
            "Epoch 2/20, Sequence 700/1000 - loss: 5.738867282867432\n",
            "Epoch 2/20, Sequence 710/1000 - loss: 6.335604190826416\n",
            "Epoch 2/20, Sequence 720/1000 - loss: 6.228543758392334\n",
            "Epoch 2/20, Sequence 730/1000 - loss: 6.120095252990723\n",
            "Epoch 2/20, Sequence 740/1000 - loss: 6.133769512176514\n",
            "Epoch 2/20, Sequence 750/1000 - loss: 6.020832538604736\n",
            "Epoch 2/20, Sequence 760/1000 - loss: 6.461203575134277\n",
            "Epoch 2/20, Sequence 770/1000 - loss: 6.277806758880615\n",
            "Epoch 2/20, Sequence 780/1000 - loss: 6.177900314331055\n",
            "Epoch 2/20, Sequence 790/1000 - loss: 6.247334957122803\n",
            "Epoch 2/20, Sequence 800/1000 - loss: 5.914417266845703\n",
            "Epoch 2/20, Sequence 810/1000 - loss: 6.3834004402160645\n",
            "Epoch 2/20, Sequence 820/1000 - loss: 5.772334575653076\n",
            "Epoch 2/20, Sequence 830/1000 - loss: 6.304502964019775\n",
            "Epoch 2/20, Sequence 840/1000 - loss: 6.1317620277404785\n",
            "Epoch 2/20, Sequence 850/1000 - loss: 5.664982795715332\n",
            "Epoch 2/20, Sequence 860/1000 - loss: 6.201809883117676\n",
            "Epoch 2/20, Sequence 870/1000 - loss: 6.151977062225342\n",
            "Epoch 2/20, Sequence 880/1000 - loss: 6.11909294128418\n",
            "Epoch 2/20, Sequence 890/1000 - loss: 6.128700256347656\n",
            "Epoch 2/20, Sequence 900/1000 - loss: 5.608243942260742\n",
            "Epoch 2/20, Sequence 910/1000 - loss: 6.034704685211182\n",
            "Epoch 2/20, Sequence 920/1000 - loss: 6.421395301818848\n",
            "Epoch 2/20, Sequence 930/1000 - loss: 6.29383659362793\n",
            "Epoch 2/20, Sequence 940/1000 - loss: 6.188638210296631\n",
            "Epoch 2/20, Sequence 950/1000 - loss: 6.059370040893555\n",
            "Epoch 2/20, Sequence 960/1000 - loss: 6.175912857055664\n",
            "Epoch 2/20, Sequence 970/1000 - loss: 6.537017345428467\n",
            "Epoch 2/20, Sequence 980/1000 - loss: 6.164910316467285\n",
            "Epoch 2/20, Sequence 990/1000 - loss: 6.045744895935059\n",
            "Epoch 3/20, Sequence 0/1000 - loss: 6.384359359741211\n",
            "Epoch 3/20, Sequence 10/1000 - loss: 6.32636833190918\n",
            "Epoch 3/20, Sequence 20/1000 - loss: 6.02115535736084\n",
            "Epoch 3/20, Sequence 30/1000 - loss: 6.36746072769165\n",
            "Epoch 3/20, Sequence 40/1000 - loss: 5.970807075500488\n",
            "Epoch 3/20, Sequence 50/1000 - loss: 6.251883029937744\n",
            "Epoch 3/20, Sequence 60/1000 - loss: 5.928706169128418\n",
            "Epoch 3/20, Sequence 70/1000 - loss: 6.460088729858398\n",
            "Epoch 3/20, Sequence 80/1000 - loss: 6.058098316192627\n",
            "Epoch 3/20, Sequence 90/1000 - loss: 6.1367998123168945\n",
            "Epoch 3/20, Sequence 100/1000 - loss: 5.891628265380859\n",
            "Epoch 3/20, Sequence 110/1000 - loss: 6.174497127532959\n",
            "Epoch 3/20, Sequence 120/1000 - loss: 6.215879440307617\n",
            "Epoch 3/20, Sequence 130/1000 - loss: 5.907505989074707\n",
            "Epoch 3/20, Sequence 140/1000 - loss: 6.11767578125\n",
            "Epoch 3/20, Sequence 150/1000 - loss: 6.165474891662598\n",
            "Epoch 3/20, Sequence 160/1000 - loss: 5.91771125793457\n",
            "Epoch 3/20, Sequence 170/1000 - loss: 5.957114219665527\n",
            "Epoch 3/20, Sequence 180/1000 - loss: 5.761261463165283\n",
            "Epoch 3/20, Sequence 190/1000 - loss: 6.087177753448486\n",
            "Epoch 3/20, Sequence 200/1000 - loss: 6.249874114990234\n",
            "Epoch 3/20, Sequence 210/1000 - loss: 5.702911853790283\n",
            "Epoch 3/20, Sequence 220/1000 - loss: 6.279053688049316\n",
            "Epoch 3/20, Sequence 230/1000 - loss: 6.071356296539307\n",
            "Epoch 3/20, Sequence 240/1000 - loss: 6.042690277099609\n",
            "Epoch 3/20, Sequence 250/1000 - loss: 6.007750988006592\n",
            "Epoch 3/20, Sequence 260/1000 - loss: 5.8085198402404785\n",
            "Epoch 3/20, Sequence 270/1000 - loss: 5.861847400665283\n",
            "Epoch 3/20, Sequence 280/1000 - loss: 5.9426374435424805\n",
            "Epoch 3/20, Sequence 290/1000 - loss: 6.040537357330322\n",
            "Epoch 3/20, Sequence 300/1000 - loss: 5.915781497955322\n",
            "Epoch 3/20, Sequence 310/1000 - loss: 5.604387283325195\n",
            "Epoch 3/20, Sequence 320/1000 - loss: 6.052661418914795\n",
            "Epoch 3/20, Sequence 330/1000 - loss: 5.814786911010742\n",
            "Epoch 3/20, Sequence 340/1000 - loss: 6.167372703552246\n",
            "Epoch 3/20, Sequence 350/1000 - loss: 6.136659145355225\n",
            "Epoch 3/20, Sequence 360/1000 - loss: 6.073637008666992\n",
            "Epoch 3/20, Sequence 370/1000 - loss: 5.9766526222229\n",
            "Epoch 3/20, Sequence 380/1000 - loss: 5.825133323669434\n",
            "Epoch 3/20, Sequence 390/1000 - loss: 6.135534286499023\n",
            "Epoch 3/20, Sequence 400/1000 - loss: 5.7801833152771\n",
            "Epoch 3/20, Sequence 410/1000 - loss: 6.221487045288086\n",
            "Epoch 3/20, Sequence 420/1000 - loss: 6.1720075607299805\n",
            "Epoch 3/20, Sequence 430/1000 - loss: 6.0521626472473145\n",
            "Epoch 3/20, Sequence 440/1000 - loss: 5.819169998168945\n",
            "Epoch 3/20, Sequence 450/1000 - loss: 5.784030437469482\n",
            "Epoch 3/20, Sequence 460/1000 - loss: 6.016536235809326\n",
            "Epoch 3/20, Sequence 470/1000 - loss: 5.819706439971924\n",
            "Epoch 3/20, Sequence 480/1000 - loss: 5.770327568054199\n",
            "Epoch 3/20, Sequence 490/1000 - loss: 5.898942470550537\n",
            "Epoch 3/20, Sequence 500/1000 - loss: 6.114775657653809\n",
            "Epoch 3/20, Sequence 510/1000 - loss: 6.132839679718018\n",
            "Epoch 3/20, Sequence 520/1000 - loss: 6.258152961730957\n",
            "Epoch 3/20, Sequence 530/1000 - loss: 5.970891952514648\n",
            "Epoch 3/20, Sequence 540/1000 - loss: 6.350020885467529\n",
            "Epoch 3/20, Sequence 550/1000 - loss: 5.861413478851318\n",
            "Epoch 3/20, Sequence 560/1000 - loss: 5.691592216491699\n",
            "Epoch 3/20, Sequence 570/1000 - loss: 5.897974491119385\n",
            "Epoch 3/20, Sequence 580/1000 - loss: 5.842401027679443\n",
            "Epoch 3/20, Sequence 590/1000 - loss: 6.113223552703857\n",
            "Epoch 3/20, Sequence 600/1000 - loss: 6.305829048156738\n",
            "Epoch 3/20, Sequence 610/1000 - loss: 5.8436479568481445\n",
            "Epoch 3/20, Sequence 620/1000 - loss: 5.83875036239624\n",
            "Epoch 3/20, Sequence 630/1000 - loss: 6.043124675750732\n",
            "Epoch 3/20, Sequence 640/1000 - loss: 6.1442766189575195\n",
            "Epoch 3/20, Sequence 650/1000 - loss: 5.984762191772461\n",
            "Epoch 3/20, Sequence 660/1000 - loss: 6.127721786499023\n",
            "Epoch 3/20, Sequence 670/1000 - loss: 6.069748401641846\n",
            "Epoch 3/20, Sequence 680/1000 - loss: 5.565999507904053\n",
            "Epoch 3/20, Sequence 690/1000 - loss: 5.520203590393066\n",
            "Epoch 3/20, Sequence 700/1000 - loss: 5.600042343139648\n",
            "Epoch 3/20, Sequence 710/1000 - loss: 6.1267876625061035\n",
            "Epoch 3/20, Sequence 720/1000 - loss: 6.030069828033447\n",
            "Epoch 3/20, Sequence 730/1000 - loss: 5.955502510070801\n",
            "Epoch 3/20, Sequence 740/1000 - loss: 5.918907642364502\n",
            "Epoch 3/20, Sequence 750/1000 - loss: 5.853726863861084\n",
            "Epoch 3/20, Sequence 760/1000 - loss: 6.265946865081787\n",
            "Epoch 3/20, Sequence 770/1000 - loss: 6.105147361755371\n",
            "Epoch 3/20, Sequence 780/1000 - loss: 6.0089111328125\n",
            "Epoch 3/20, Sequence 790/1000 - loss: 6.107329845428467\n",
            "Epoch 3/20, Sequence 800/1000 - loss: 5.72816801071167\n",
            "Epoch 3/20, Sequence 810/1000 - loss: 6.208644390106201\n",
            "Epoch 3/20, Sequence 820/1000 - loss: 5.542349815368652\n",
            "Epoch 3/20, Sequence 830/1000 - loss: 6.158507347106934\n",
            "Epoch 3/20, Sequence 840/1000 - loss: 5.96265172958374\n",
            "Epoch 3/20, Sequence 850/1000 - loss: 5.449332237243652\n",
            "Epoch 3/20, Sequence 860/1000 - loss: 6.0167436599731445\n",
            "Epoch 3/20, Sequence 870/1000 - loss: 6.000435829162598\n",
            "Epoch 3/20, Sequence 880/1000 - loss: 5.938129901885986\n",
            "Epoch 3/20, Sequence 890/1000 - loss: 5.930171489715576\n",
            "Epoch 3/20, Sequence 900/1000 - loss: 5.432100772857666\n",
            "Epoch 3/20, Sequence 910/1000 - loss: 5.844006538391113\n",
            "Epoch 3/20, Sequence 920/1000 - loss: 6.2189178466796875\n",
            "Epoch 3/20, Sequence 930/1000 - loss: 6.1750898361206055\n",
            "Epoch 3/20, Sequence 940/1000 - loss: 6.019617080688477\n",
            "Epoch 3/20, Sequence 950/1000 - loss: 5.8801960945129395\n",
            "Epoch 3/20, Sequence 960/1000 - loss: 6.044239521026611\n",
            "Epoch 3/20, Sequence 970/1000 - loss: 6.340071201324463\n",
            "Epoch 3/20, Sequence 980/1000 - loss: 5.960153102874756\n",
            "Epoch 3/20, Sequence 990/1000 - loss: 5.894310474395752\n",
            "Epoch 4/20, Sequence 0/1000 - loss: 6.24156379699707\n",
            "Epoch 4/20, Sequence 10/1000 - loss: 6.205667495727539\n",
            "Epoch 4/20, Sequence 20/1000 - loss: 5.762284755706787\n",
            "Epoch 4/20, Sequence 30/1000 - loss: 6.227113246917725\n",
            "Epoch 4/20, Sequence 40/1000 - loss: 5.833108425140381\n",
            "Epoch 4/20, Sequence 50/1000 - loss: 6.070812225341797\n",
            "Epoch 4/20, Sequence 60/1000 - loss: 5.758739948272705\n",
            "Epoch 4/20, Sequence 70/1000 - loss: 6.304771423339844\n",
            "Epoch 4/20, Sequence 80/1000 - loss: 5.874500274658203\n",
            "Epoch 4/20, Sequence 90/1000 - loss: 5.945816516876221\n",
            "Epoch 4/20, Sequence 100/1000 - loss: 5.665729999542236\n",
            "Epoch 4/20, Sequence 110/1000 - loss: 6.020932197570801\n",
            "Epoch 4/20, Sequence 120/1000 - loss: 6.083711624145508\n",
            "Epoch 4/20, Sequence 130/1000 - loss: 5.677369117736816\n",
            "Epoch 4/20, Sequence 140/1000 - loss: 5.964074611663818\n",
            "Epoch 4/20, Sequence 150/1000 - loss: 5.986573219299316\n",
            "Epoch 4/20, Sequence 160/1000 - loss: 5.746312141418457\n",
            "Epoch 4/20, Sequence 170/1000 - loss: 5.712296962738037\n",
            "Epoch 4/20, Sequence 180/1000 - loss: 5.5938591957092285\n",
            "Epoch 4/20, Sequence 190/1000 - loss: 5.910711288452148\n",
            "Epoch 4/20, Sequence 200/1000 - loss: 6.101414203643799\n",
            "Epoch 4/20, Sequence 210/1000 - loss: 5.509042263031006\n",
            "Epoch 4/20, Sequence 220/1000 - loss: 6.176788806915283\n",
            "Epoch 4/20, Sequence 230/1000 - loss: 5.85099983215332\n",
            "Epoch 4/20, Sequence 240/1000 - loss: 5.877569675445557\n",
            "Epoch 4/20, Sequence 250/1000 - loss: 5.9209418296813965\n",
            "Epoch 4/20, Sequence 260/1000 - loss: 5.654110908508301\n",
            "Epoch 4/20, Sequence 270/1000 - loss: 5.6270751953125\n",
            "Epoch 4/20, Sequence 280/1000 - loss: 5.674756050109863\n",
            "Epoch 4/20, Sequence 290/1000 - loss: 5.887650012969971\n",
            "Epoch 4/20, Sequence 300/1000 - loss: 5.765015602111816\n",
            "Epoch 4/20, Sequence 310/1000 - loss: 5.440910339355469\n",
            "Epoch 4/20, Sequence 320/1000 - loss: 5.869493007659912\n",
            "Epoch 4/20, Sequence 330/1000 - loss: 5.633005142211914\n",
            "Epoch 4/20, Sequence 340/1000 - loss: 6.027990818023682\n",
            "Epoch 4/20, Sequence 350/1000 - loss: 5.901157379150391\n",
            "Epoch 4/20, Sequence 360/1000 - loss: 5.915812015533447\n",
            "Epoch 4/20, Sequence 370/1000 - loss: 5.836101055145264\n",
            "Epoch 4/20, Sequence 380/1000 - loss: 5.592427730560303\n",
            "Epoch 4/20, Sequence 390/1000 - loss: 5.882259368896484\n",
            "Epoch 4/20, Sequence 400/1000 - loss: 5.551567077636719\n",
            "Epoch 4/20, Sequence 410/1000 - loss: 6.093747138977051\n",
            "Epoch 4/20, Sequence 420/1000 - loss: 5.994765758514404\n",
            "Epoch 4/20, Sequence 430/1000 - loss: 5.786322593688965\n",
            "Epoch 4/20, Sequence 440/1000 - loss: 5.664280891418457\n",
            "Epoch 4/20, Sequence 450/1000 - loss: 5.590015411376953\n",
            "Epoch 4/20, Sequence 460/1000 - loss: 5.893462657928467\n",
            "Epoch 4/20, Sequence 470/1000 - loss: 5.689653396606445\n",
            "Epoch 4/20, Sequence 480/1000 - loss: 5.534117221832275\n",
            "Epoch 4/20, Sequence 490/1000 - loss: 5.788307189941406\n",
            "Epoch 4/20, Sequence 500/1000 - loss: 5.972200393676758\n",
            "Epoch 4/20, Sequence 510/1000 - loss: 5.96208381652832\n",
            "Epoch 4/20, Sequence 520/1000 - loss: 6.079108715057373\n",
            "Epoch 4/20, Sequence 530/1000 - loss: 5.800013065338135\n",
            "Epoch 4/20, Sequence 540/1000 - loss: 6.22966194152832\n",
            "Epoch 4/20, Sequence 550/1000 - loss: 5.697225093841553\n",
            "Epoch 4/20, Sequence 560/1000 - loss: 5.586349010467529\n",
            "Epoch 4/20, Sequence 570/1000 - loss: 5.73130464553833\n",
            "Epoch 4/20, Sequence 580/1000 - loss: 5.691461563110352\n",
            "Epoch 4/20, Sequence 590/1000 - loss: 5.977093696594238\n",
            "Epoch 4/20, Sequence 600/1000 - loss: 6.220850944519043\n",
            "Epoch 4/20, Sequence 610/1000 - loss: 5.625807285308838\n",
            "Epoch 4/20, Sequence 620/1000 - loss: 5.7223896980285645\n",
            "Epoch 4/20, Sequence 630/1000 - loss: 5.869187355041504\n",
            "Epoch 4/20, Sequence 640/1000 - loss: 5.879750728607178\n",
            "Epoch 4/20, Sequence 650/1000 - loss: 5.816319942474365\n",
            "Epoch 4/20, Sequence 660/1000 - loss: 6.0268874168396\n",
            "Epoch 4/20, Sequence 670/1000 - loss: 5.90731143951416\n",
            "Epoch 4/20, Sequence 680/1000 - loss: 5.415780544281006\n",
            "Epoch 4/20, Sequence 690/1000 - loss: 5.307470798492432\n",
            "Epoch 4/20, Sequence 700/1000 - loss: 5.486450672149658\n",
            "Epoch 4/20, Sequence 710/1000 - loss: 5.943704605102539\n",
            "Epoch 4/20, Sequence 720/1000 - loss: 5.888265132904053\n",
            "Epoch 4/20, Sequence 730/1000 - loss: 5.8401288986206055\n",
            "Epoch 4/20, Sequence 740/1000 - loss: 5.755901336669922\n",
            "Epoch 4/20, Sequence 750/1000 - loss: 5.735005855560303\n",
            "Epoch 4/20, Sequence 760/1000 - loss: 6.014634609222412\n",
            "Epoch 4/20, Sequence 770/1000 - loss: 5.949484825134277\n",
            "Epoch 4/20, Sequence 780/1000 - loss: 5.856019496917725\n",
            "Epoch 4/20, Sequence 790/1000 - loss: 5.986179351806641\n",
            "Epoch 4/20, Sequence 800/1000 - loss: 5.549999713897705\n",
            "Epoch 4/20, Sequence 810/1000 - loss: 6.072811603546143\n",
            "Epoch 4/20, Sequence 820/1000 - loss: 5.376762866973877\n",
            "Epoch 4/20, Sequence 830/1000 - loss: 6.001163482666016\n",
            "Epoch 4/20, Sequence 840/1000 - loss: 5.797942638397217\n",
            "Epoch 4/20, Sequence 850/1000 - loss: 5.276425361633301\n",
            "Epoch 4/20, Sequence 860/1000 - loss: 5.889458656311035\n",
            "Epoch 4/20, Sequence 870/1000 - loss: 5.869438171386719\n",
            "Epoch 4/20, Sequence 880/1000 - loss: 5.766473770141602\n",
            "Epoch 4/20, Sequence 890/1000 - loss: 5.774302959442139\n",
            "Epoch 4/20, Sequence 900/1000 - loss: 5.324841022491455\n",
            "Epoch 4/20, Sequence 910/1000 - loss: 5.689161777496338\n",
            "Epoch 4/20, Sequence 920/1000 - loss: 6.036757469177246\n",
            "Epoch 4/20, Sequence 930/1000 - loss: 6.033060073852539\n",
            "Epoch 4/20, Sequence 940/1000 - loss: 5.883333206176758\n",
            "Epoch 4/20, Sequence 950/1000 - loss: 5.716380596160889\n",
            "Epoch 4/20, Sequence 960/1000 - loss: 5.915147304534912\n",
            "Epoch 4/20, Sequence 970/1000 - loss: 6.1812214851379395\n",
            "Epoch 4/20, Sequence 980/1000 - loss: 5.767782211303711\n",
            "Epoch 4/20, Sequence 990/1000 - loss: 5.768742561340332\n",
            "Epoch 5/20, Sequence 0/1000 - loss: 6.120541572570801\n",
            "Epoch 5/20, Sequence 10/1000 - loss: 6.078898906707764\n",
            "Epoch 5/20, Sequence 20/1000 - loss: 5.541619300842285\n",
            "Epoch 5/20, Sequence 30/1000 - loss: 6.113002300262451\n",
            "Epoch 5/20, Sequence 40/1000 - loss: 5.723505020141602\n",
            "Epoch 5/20, Sequence 50/1000 - loss: 5.907401084899902\n",
            "Epoch 5/20, Sequence 60/1000 - loss: 5.638803005218506\n",
            "Epoch 5/20, Sequence 70/1000 - loss: 6.154363632202148\n",
            "Epoch 5/20, Sequence 80/1000 - loss: 5.738831996917725\n",
            "Epoch 5/20, Sequence 90/1000 - loss: 5.760920524597168\n",
            "Epoch 5/20, Sequence 100/1000 - loss: 5.501070022583008\n",
            "Epoch 5/20, Sequence 110/1000 - loss: 5.9170241355896\n",
            "Epoch 5/20, Sequence 120/1000 - loss: 5.977768898010254\n",
            "Epoch 5/20, Sequence 130/1000 - loss: 5.4936370849609375\n",
            "Epoch 5/20, Sequence 140/1000 - loss: 5.828664779663086\n",
            "Epoch 5/20, Sequence 150/1000 - loss: 5.832137107849121\n",
            "Epoch 5/20, Sequence 160/1000 - loss: 5.643908500671387\n",
            "Epoch 5/20, Sequence 170/1000 - loss: 5.533207416534424\n",
            "Epoch 5/20, Sequence 180/1000 - loss: 5.47951602935791\n",
            "Epoch 5/20, Sequence 190/1000 - loss: 5.736884117126465\n",
            "Epoch 5/20, Sequence 200/1000 - loss: 5.963943004608154\n",
            "Epoch 5/20, Sequence 210/1000 - loss: 5.37807035446167\n",
            "Epoch 5/20, Sequence 220/1000 - loss: 6.077507972717285\n",
            "Epoch 5/20, Sequence 230/1000 - loss: 5.561633110046387\n",
            "Epoch 5/20, Sequence 240/1000 - loss: 5.753236293792725\n",
            "Epoch 5/20, Sequence 250/1000 - loss: 5.829225063323975\n",
            "Epoch 5/20, Sequence 260/1000 - loss: 5.490417957305908\n",
            "Epoch 5/20, Sequence 270/1000 - loss: 5.349153518676758\n",
            "Epoch 5/20, Sequence 280/1000 - loss: 5.3836541175842285\n",
            "Epoch 5/20, Sequence 290/1000 - loss: 5.757134437561035\n",
            "Epoch 5/20, Sequence 300/1000 - loss: 5.639631748199463\n",
            "Epoch 5/20, Sequence 310/1000 - loss: 5.299899101257324\n",
            "Epoch 5/20, Sequence 320/1000 - loss: 5.733834266662598\n",
            "Epoch 5/20, Sequence 330/1000 - loss: 5.43379545211792\n",
            "Epoch 5/20, Sequence 340/1000 - loss: 5.864862442016602\n",
            "Epoch 5/20, Sequence 350/1000 - loss: 5.678613662719727\n",
            "Epoch 5/20, Sequence 360/1000 - loss: 5.770501136779785\n",
            "Epoch 5/20, Sequence 370/1000 - loss: 5.743654727935791\n",
            "Epoch 5/20, Sequence 380/1000 - loss: 5.437238693237305\n",
            "Epoch 5/20, Sequence 390/1000 - loss: 5.667673110961914\n",
            "Epoch 5/20, Sequence 400/1000 - loss: 5.3261399269104\n",
            "Epoch 5/20, Sequence 410/1000 - loss: 5.9696574211120605\n",
            "Epoch 5/20, Sequence 420/1000 - loss: 5.850795745849609\n",
            "Epoch 5/20, Sequence 430/1000 - loss: 5.564184188842773\n",
            "Epoch 5/20, Sequence 440/1000 - loss: 5.561554431915283\n",
            "Epoch 5/20, Sequence 450/1000 - loss: 5.372135639190674\n",
            "Epoch 5/20, Sequence 460/1000 - loss: 5.786113262176514\n",
            "Epoch 5/20, Sequence 470/1000 - loss: 5.61273717880249\n",
            "Epoch 5/20, Sequence 480/1000 - loss: 5.354205131530762\n",
            "Epoch 5/20, Sequence 490/1000 - loss: 5.679014205932617\n",
            "Epoch 5/20, Sequence 500/1000 - loss: 5.855881214141846\n",
            "Epoch 5/20, Sequence 510/1000 - loss: 5.806526184082031\n",
            "Epoch 5/20, Sequence 520/1000 - loss: 5.9439473152160645\n",
            "Epoch 5/20, Sequence 530/1000 - loss: 5.670160293579102\n",
            "Epoch 5/20, Sequence 540/1000 - loss: 6.111666679382324\n",
            "Epoch 5/20, Sequence 550/1000 - loss: 5.559641361236572\n",
            "Epoch 5/20, Sequence 560/1000 - loss: 5.520239353179932\n",
            "Epoch 5/20, Sequence 570/1000 - loss: 5.589221477508545\n",
            "Epoch 5/20, Sequence 580/1000 - loss: 5.5308732986450195\n",
            "Epoch 5/20, Sequence 590/1000 - loss: 5.845576763153076\n",
            "Epoch 5/20, Sequence 600/1000 - loss: 6.138089179992676\n",
            "Epoch 5/20, Sequence 610/1000 - loss: 5.440662384033203\n",
            "Epoch 5/20, Sequence 620/1000 - loss: 5.620419025421143\n",
            "Epoch 5/20, Sequence 630/1000 - loss: 5.7039690017700195\n",
            "Epoch 5/20, Sequence 640/1000 - loss: 5.645594120025635\n",
            "Epoch 5/20, Sequence 650/1000 - loss: 5.674557209014893\n",
            "Epoch 5/20, Sequence 660/1000 - loss: 5.966305732727051\n",
            "Epoch 5/20, Sequence 670/1000 - loss: 5.774697303771973\n",
            "Epoch 5/20, Sequence 680/1000 - loss: 5.292321681976318\n",
            "Epoch 5/20, Sequence 690/1000 - loss: 5.104816436767578\n",
            "Epoch 5/20, Sequence 700/1000 - loss: 5.405553817749023\n",
            "Epoch 5/20, Sequence 710/1000 - loss: 5.821656227111816\n",
            "Epoch 5/20, Sequence 720/1000 - loss: 5.778824806213379\n",
            "Epoch 5/20, Sequence 730/1000 - loss: 5.734890460968018\n",
            "Epoch 5/20, Sequence 740/1000 - loss: 5.609830856323242\n",
            "Epoch 5/20, Sequence 750/1000 - loss: 5.601365089416504\n",
            "Epoch 5/20, Sequence 760/1000 - loss: 5.760542392730713\n",
            "Epoch 5/20, Sequence 770/1000 - loss: 5.827029228210449\n",
            "Epoch 5/20, Sequence 780/1000 - loss: 5.743335247039795\n",
            "Epoch 5/20, Sequence 790/1000 - loss: 5.873616695404053\n",
            "Epoch 5/20, Sequence 800/1000 - loss: 5.390008926391602\n",
            "Epoch 5/20, Sequence 810/1000 - loss: 5.942117691040039\n",
            "Epoch 5/20, Sequence 820/1000 - loss: 5.215992450714111\n",
            "Epoch 5/20, Sequence 830/1000 - loss: 5.842978000640869\n",
            "Epoch 5/20, Sequence 840/1000 - loss: 5.662496089935303\n",
            "Epoch 5/20, Sequence 850/1000 - loss: 5.105959415435791\n",
            "Epoch 5/20, Sequence 860/1000 - loss: 5.800459861755371\n",
            "Epoch 5/20, Sequence 870/1000 - loss: 5.756402015686035\n",
            "Epoch 5/20, Sequence 880/1000 - loss: 5.626248836517334\n",
            "Epoch 5/20, Sequence 890/1000 - loss: 5.62476110458374\n",
            "Epoch 5/20, Sequence 900/1000 - loss: 5.242771625518799\n",
            "Epoch 5/20, Sequence 910/1000 - loss: 5.5641255378723145\n",
            "Epoch 5/20, Sequence 920/1000 - loss: 5.910680294036865\n",
            "Epoch 5/20, Sequence 930/1000 - loss: 5.936168670654297\n",
            "Epoch 5/20, Sequence 940/1000 - loss: 5.765227794647217\n",
            "Epoch 5/20, Sequence 950/1000 - loss: 5.555391311645508\n",
            "Epoch 5/20, Sequence 960/1000 - loss: 5.785372734069824\n",
            "Epoch 5/20, Sequence 970/1000 - loss: 6.0458221435546875\n",
            "Epoch 5/20, Sequence 980/1000 - loss: 5.566792964935303\n",
            "Epoch 5/20, Sequence 990/1000 - loss: 5.648831367492676\n",
            "Epoch 6/20, Sequence 0/1000 - loss: 6.020003795623779\n",
            "Epoch 6/20, Sequence 10/1000 - loss: 5.9664692878723145\n",
            "Epoch 6/20, Sequence 20/1000 - loss: 5.390007019042969\n",
            "Epoch 6/20, Sequence 30/1000 - loss: 6.021643161773682\n",
            "Epoch 6/20, Sequence 40/1000 - loss: 5.608553886413574\n",
            "Epoch 6/20, Sequence 50/1000 - loss: 5.741861820220947\n",
            "Epoch 6/20, Sequence 60/1000 - loss: 5.537048816680908\n",
            "Epoch 6/20, Sequence 70/1000 - loss: 6.009352684020996\n",
            "Epoch 6/20, Sequence 80/1000 - loss: 5.594546318054199\n",
            "Epoch 6/20, Sequence 90/1000 - loss: 5.60137939453125\n",
            "Epoch 6/20, Sequence 100/1000 - loss: 5.349178314208984\n",
            "Epoch 6/20, Sequence 110/1000 - loss: 5.844823360443115\n",
            "Epoch 6/20, Sequence 120/1000 - loss: 5.881714344024658\n",
            "Epoch 6/20, Sequence 130/1000 - loss: 5.326976299285889\n",
            "Epoch 6/20, Sequence 140/1000 - loss: 5.72704553604126\n",
            "Epoch 6/20, Sequence 150/1000 - loss: 5.696865081787109\n",
            "Epoch 6/20, Sequence 160/1000 - loss: 5.5310564041137695\n",
            "Epoch 6/20, Sequence 170/1000 - loss: 5.335872173309326\n",
            "Epoch 6/20, Sequence 180/1000 - loss: 5.401136875152588\n",
            "Epoch 6/20, Sequence 190/1000 - loss: 5.586283206939697\n",
            "Epoch 6/20, Sequence 200/1000 - loss: 5.832821846008301\n",
            "Epoch 6/20, Sequence 210/1000 - loss: 5.233292102813721\n",
            "Epoch 6/20, Sequence 220/1000 - loss: 6.002267360687256\n",
            "Epoch 6/20, Sequence 230/1000 - loss: 5.334092617034912\n",
            "Epoch 6/20, Sequence 240/1000 - loss: 5.676886558532715\n",
            "Epoch 6/20, Sequence 250/1000 - loss: 5.764294624328613\n",
            "Epoch 6/20, Sequence 260/1000 - loss: 5.34106969833374\n",
            "Epoch 6/20, Sequence 270/1000 - loss: 5.096060276031494\n",
            "Epoch 6/20, Sequence 280/1000 - loss: 5.135016441345215\n",
            "Epoch 6/20, Sequence 290/1000 - loss: 5.651561260223389\n",
            "Epoch 6/20, Sequence 300/1000 - loss: 5.520389556884766\n",
            "Epoch 6/20, Sequence 310/1000 - loss: 5.189571857452393\n",
            "Epoch 6/20, Sequence 320/1000 - loss: 5.635178565979004\n",
            "Epoch 6/20, Sequence 330/1000 - loss: 5.296200752258301\n",
            "Epoch 6/20, Sequence 340/1000 - loss: 5.766850471496582\n",
            "Epoch 6/20, Sequence 350/1000 - loss: 5.501461982727051\n",
            "Epoch 6/20, Sequence 360/1000 - loss: 5.632615089416504\n",
            "Epoch 6/20, Sequence 370/1000 - loss: 5.630830764770508\n",
            "Epoch 6/20, Sequence 380/1000 - loss: 5.317431449890137\n",
            "Epoch 6/20, Sequence 390/1000 - loss: 5.494115829467773\n",
            "Epoch 6/20, Sequence 400/1000 - loss: 5.150335311889648\n",
            "Epoch 6/20, Sequence 410/1000 - loss: 5.869863510131836\n",
            "Epoch 6/20, Sequence 420/1000 - loss: 5.721245765686035\n",
            "Epoch 6/20, Sequence 430/1000 - loss: 5.316318988800049\n",
            "Epoch 6/20, Sequence 440/1000 - loss: 5.448610305786133\n",
            "Epoch 6/20, Sequence 450/1000 - loss: 5.230122089385986\n",
            "Epoch 6/20, Sequence 460/1000 - loss: 5.690544605255127\n",
            "Epoch 6/20, Sequence 470/1000 - loss: 5.516079902648926\n",
            "Epoch 6/20, Sequence 480/1000 - loss: 5.200291633605957\n",
            "Epoch 6/20, Sequence 490/1000 - loss: 5.598503112792969\n",
            "Epoch 6/20, Sequence 500/1000 - loss: 5.724600315093994\n",
            "Epoch 6/20, Sequence 510/1000 - loss: 5.6798553466796875\n",
            "Epoch 6/20, Sequence 520/1000 - loss: 5.819638252258301\n",
            "Epoch 6/20, Sequence 530/1000 - loss: 5.550908088684082\n",
            "Epoch 6/20, Sequence 540/1000 - loss: 6.000793933868408\n",
            "Epoch 6/20, Sequence 550/1000 - loss: 5.432494163513184\n",
            "Epoch 6/20, Sequence 560/1000 - loss: 5.473727226257324\n",
            "Epoch 6/20, Sequence 570/1000 - loss: 5.474496364593506\n",
            "Epoch 6/20, Sequence 580/1000 - loss: 5.398400783538818\n",
            "Epoch 6/20, Sequence 590/1000 - loss: 5.719720840454102\n",
            "Epoch 6/20, Sequence 600/1000 - loss: 6.076547145843506\n",
            "Epoch 6/20, Sequence 610/1000 - loss: 5.320860862731934\n",
            "Epoch 6/20, Sequence 620/1000 - loss: 5.549956798553467\n",
            "Epoch 6/20, Sequence 630/1000 - loss: 5.584310054779053\n",
            "Epoch 6/20, Sequence 640/1000 - loss: 5.393799781799316\n",
            "Epoch 6/20, Sequence 650/1000 - loss: 5.548608303070068\n",
            "Epoch 6/20, Sequence 660/1000 - loss: 5.909844875335693\n",
            "Epoch 6/20, Sequence 670/1000 - loss: 5.6300950050354\n",
            "Epoch 6/20, Sequence 680/1000 - loss: 5.195271968841553\n",
            "Epoch 6/20, Sequence 690/1000 - loss: 4.932961940765381\n",
            "Epoch 6/20, Sequence 700/1000 - loss: 5.346214771270752\n",
            "Epoch 6/20, Sequence 710/1000 - loss: 5.708983421325684\n",
            "Epoch 6/20, Sequence 720/1000 - loss: 5.686211109161377\n",
            "Epoch 6/20, Sequence 730/1000 - loss: 5.643823146820068\n",
            "Epoch 6/20, Sequence 740/1000 - loss: 5.521717548370361\n",
            "Epoch 6/20, Sequence 750/1000 - loss: 5.512051582336426\n",
            "Epoch 6/20, Sequence 760/1000 - loss: 5.530201435089111\n",
            "Epoch 6/20, Sequence 770/1000 - loss: 5.703794002532959\n",
            "Epoch 6/20, Sequence 780/1000 - loss: 5.631592273712158\n",
            "Epoch 6/20, Sequence 790/1000 - loss: 5.771543979644775\n",
            "Epoch 6/20, Sequence 800/1000 - loss: 5.269735336303711\n",
            "Epoch 6/20, Sequence 810/1000 - loss: 5.821728706359863\n",
            "Epoch 6/20, Sequence 820/1000 - loss: 5.0776801109313965\n",
            "Epoch 6/20, Sequence 830/1000 - loss: 5.711449146270752\n",
            "Epoch 6/20, Sequence 840/1000 - loss: 5.537866592407227\n",
            "Epoch 6/20, Sequence 850/1000 - loss: 4.939672946929932\n",
            "Epoch 6/20, Sequence 860/1000 - loss: 5.731790542602539\n",
            "Epoch 6/20, Sequence 870/1000 - loss: 5.660497665405273\n",
            "Epoch 6/20, Sequence 880/1000 - loss: 5.5254364013671875\n",
            "Epoch 6/20, Sequence 890/1000 - loss: 5.493129253387451\n",
            "Epoch 6/20, Sequence 900/1000 - loss: 5.178380489349365\n",
            "Epoch 6/20, Sequence 910/1000 - loss: 5.446603298187256\n",
            "Epoch 6/20, Sequence 920/1000 - loss: 5.792762279510498\n",
            "Epoch 6/20, Sequence 930/1000 - loss: 5.83193826675415\n",
            "Epoch 6/20, Sequence 940/1000 - loss: 5.651129722595215\n",
            "Epoch 6/20, Sequence 950/1000 - loss: 5.430980682373047\n",
            "Epoch 6/20, Sequence 960/1000 - loss: 5.714369297027588\n",
            "Epoch 6/20, Sequence 970/1000 - loss: 5.918768405914307\n",
            "Epoch 6/20, Sequence 980/1000 - loss: 5.410011291503906\n",
            "Epoch 6/20, Sequence 990/1000 - loss: 5.566666126251221\n",
            "Epoch 7/20, Sequence 0/1000 - loss: 5.9430670738220215\n",
            "Epoch 7/20, Sequence 10/1000 - loss: 5.869406223297119\n",
            "Epoch 7/20, Sequence 20/1000 - loss: 5.2276530265808105\n",
            "Epoch 7/20, Sequence 30/1000 - loss: 5.943150520324707\n",
            "Epoch 7/20, Sequence 40/1000 - loss: 5.523262023925781\n",
            "Epoch 7/20, Sequence 50/1000 - loss: 5.630133628845215\n",
            "Epoch 7/20, Sequence 60/1000 - loss: 5.431319236755371\n",
            "Epoch 7/20, Sequence 70/1000 - loss: 5.881254196166992\n",
            "Epoch 7/20, Sequence 80/1000 - loss: 5.472379684448242\n",
            "Epoch 7/20, Sequence 90/1000 - loss: 5.4557061195373535\n",
            "Epoch 7/20, Sequence 100/1000 - loss: 5.2258124351501465\n",
            "Epoch 7/20, Sequence 110/1000 - loss: 5.77005672454834\n",
            "Epoch 7/20, Sequence 120/1000 - loss: 5.793087482452393\n",
            "Epoch 7/20, Sequence 130/1000 - loss: 5.184359073638916\n",
            "Epoch 7/20, Sequence 140/1000 - loss: 5.66241455078125\n",
            "Epoch 7/20, Sequence 150/1000 - loss: 5.588542461395264\n",
            "Epoch 7/20, Sequence 160/1000 - loss: 5.473540782928467\n",
            "Epoch 7/20, Sequence 170/1000 - loss: 5.270823001861572\n",
            "Epoch 7/20, Sequence 180/1000 - loss: 5.267964839935303\n",
            "Epoch 7/20, Sequence 190/1000 - loss: 5.451962947845459\n",
            "Epoch 7/20, Sequence 200/1000 - loss: 5.703198432922363\n",
            "Epoch 7/20, Sequence 210/1000 - loss: 5.146702289581299\n",
            "Epoch 7/20, Sequence 220/1000 - loss: 5.945356369018555\n",
            "Epoch 7/20, Sequence 230/1000 - loss: 5.131227493286133\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-f300857667a8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'<bos>{sequence}<eos>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encode: `sequence` can't be `None`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     def encode_batch(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.5. Sample from the Model"
      ],
      "metadata": {
        "id": "ZmlAGlW9UwAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "def sample_top_k(prompt: str, sequence_length: int, k: int = 5) -> str:\n",
        "    \"\"\"Samples from the model using top-k sampling.\"\"\"\n",
        "\n",
        "    prompt_tokens = torch.tensor(tokenizer.encode(f'<bos>{prompt}').ids).cuda()\n",
        "    prompt_length = len(prompt_tokens)\n",
        "\n",
        "    sequence = torch.zeros(sequence_length).to(int).cuda()\n",
        "    sequence[: prompt_length] = prompt_tokens  # Insert the prompt.\n",
        "\n",
        "    # Sample the completion.\n",
        "\n",
        "    for token_position in range(prompt_length, sequence_length):\n",
        "        logits = model(sequence.view(1, -1)).detach()\n",
        "        logits = logits[0][token_position]\n",
        "\n",
        "        top_k_logits = torch.topk(logits, k=k)\n",
        "        top_k_distribution = F.softmax(top_k_logits.values, dim=-1)\n",
        "\n",
        "        index = torch.multinomial(top_k_distribution, num_samples=1, replacement=True)\n",
        "        token = top_k_logits.indices[index]\n",
        "\n",
        "        sequence[token_position] = token\n",
        "\n",
        "    return sequence\n"
      ],
      "metadata": {
        "id": "ly6u7doyVOw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(sample_top_k('i think', 100, k=5).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "s0agKSeJMZyH",
        "outputId": "f9f80227-958e-4ef1-8e3d-05048e69b234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i think the film would be fine of the movie are the film is so really a good sense of pace.  and was a lot to be a lot of the fact that he is nowhere for the film is not as much as it is so really it would be anything to do that i was surprised that i was surprised that the film would not forget the film to get the way to see a film that will have been really forget the whole time and it is a fine job of the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    }
  ]
}